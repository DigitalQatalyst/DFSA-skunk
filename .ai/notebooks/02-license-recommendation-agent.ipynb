{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFSA License Recommendation Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts Templating for the License Recommendation Agent - LangChain Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The License Recommendation Agent is a sophisticated, AI-powered assistant designed to help new applicants and regulated entities navigate the DFSA licensing process. Built using Python for backend processing, JavaScript for frontend interactions, and REST APIs for communication, the agent provides real-time license recommendations, eligibility checks, and regulatory pathway guidance.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The License Recommendation Agent is designed to:\n",
    "\n",
    "* **Assist New Applicants**: Help businesses identify the correct DFSA license based on their sector, services, and activities.\n",
    "* **Help Regulated Entities**: Guide regulated businesses in modifying or renewing their licenses according to their updated operations.\n",
    "* **Ensure Compliance**: Validate the eligibility of applicants for the recommended license type and guide them through the regulatory process.\n",
    "* **Increase Efficiency**: Automate the process of selecting the correct license and validating eligibility, reducing manual errors and improving overall compliance.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| License Mapping | Recommends the most suitable DFSA license based on user input (business sector, activities, etc.). |\n",
    "| Eligibility Check | Verifies whether the user meets the eligibility criteria for the suggested license. |\n",
    "| Regulatory Pathway Guidance | Provides step-by-step guidance through the entire licensing process, including documentation and approvals. |\n",
    "| Real-Time Recommendations | Dynamic recommendations using data sourced directly from the DFSA Licensing Database. |\n",
    "\n",
    "## Prompting Strategy\n",
    "\n",
    "Prompts are fundamental to the License Recommendation Agent's functionality. Through natural language instructions, we guide the LLM to provide accurate license recommendations, check eligibility, and deliver regulatory guidance. LangChain enables us to build dynamic prompting pipelines that adapt based on user input, business sector, and eligibility status.\n",
    "\n",
    "In this notebook, we'll explore how to structure prompts for the License Recommendation Agent using **R**etrieval **A**ugmented **G**eneration (RAG) patterns combined with the DFSA licensing database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, please see the [Ollama version](https://github.com/aurelio-labs/langchain-course/blob/main/notebooks/ollama/02-prompts-ollama.ipynb) of this example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Prompting for License Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the License Recommendation Agent, we structure prompts with the following core components:\n",
    "\n",
    "* **System Instructions**: Defines the agent's role as a DFSA licensing expert, how it should approach recommendations, and its behavior guidelines.\n",
    "\n",
    "* **Context**: Retrieved from the DFSA Licensing Database, containing license types, eligibility criteria, and regulatory requirements relevant to the user's query.\n",
    "\n",
    "* **User Query**: The applicant's question about licensing, eligibility, or regulatory pathways.\n",
    "\n",
    "* **Agent Response**: The recommended license, eligibility status, and next steps in the licensing process.\n",
    "\n",
    "Below is an example of how a License Recommendation prompt may look:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "You are a DFSA Licensing Expert Assistant.                       }\n",
    "Provide accurate license recommendations based on the            }--->  (System Instructions)\n",
    "applicant's business sector and activities.                      }\n",
    "\n",
    "DFSA License Database:                                           }\n",
    "- Financial Services License: For companies providing             }\n",
    "  investment services, wealth management, etc.                   }\n",
    "- Insurance License: For insurance brokers and agents.           }--->  (Context from DFSA DB)\n",
    "- Freezone License: For businesses operating in DIFC.            }\n",
    "Eligibility: Minimum capital of AED 500,000.                     }\n",
    "\n",
    "User Query: I run a financial services company. What license      }--->  (User Query)\n",
    "do I need?\n",
    "\n",
    "Recommendation:                                                  }--->  (Agent Response)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how the License Recommendation Agent approaches a user query. The system instructions define the agent's expertise, the context provides relevant DFSA licensing information, the user query is the applicant's question, and the agent response includes the recommended license and next steps. This structure ensures accurate, compliant recommendations based on real DFSA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License Recommendation Agent - System Prompt Template\n",
    "system_prompt = \"\"\"\n",
    "You are an expert DFSA Licensing Assistant. Your role is to:\n",
    "1. Recommend the most suitable DFSA license based on the applicant's business sector and activities.\n",
    "2. Verify eligibility criteria for the recommended license.\n",
    "3. Provide step-by-step regulatory pathway guidance.\n",
    "4. Ensure all recommendations comply with DFSA regulations.\n",
    "\n",
    "If you cannot determine the appropriate license based on the provided information,\n",
    "ask clarifying questions about the applicant's business activities and sector.\n",
    "\n",
    "Always reference the DFSA Licensing Database for accurate information.\n",
    "\"\"\"\n",
    "\n",
    "# RAG Context Template for License Recommendations\n",
    "rag_prompt = \"\"\"\n",
    "Based on the DFSA Licensing Database context below, provide a license recommendation.\n",
    "If you cannot determine the appropriate license, ask clarifying questions.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain uses a `ChatPromptTemplate` object to format the various prompt types into a single list which will be passed to our LLM. For the License Recommendation Agent, we structure it with system instructions and user queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# License Recommendation Agent - ChatPromptTemplate\n",
    "license_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "# RAG-enabled License Recommendation Template\n",
    "rag_license_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", rag_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call the template it will expect us to provide the `query` variable. For RAG-enabled templates, we also provide `context` from the DFSA Licensing Database. LangChain interprets curly-bracket syntax (ie `{context}` and `{query}`) as indicating a dynamic variable that we expect to be inserted at query time. We can see that these variables have been picked up by our template object by viewing its `input_variables` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['query']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "license_prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the structure of the messages (currently _prompt templates_) that the `ChatPromptTemplate` will construct by viewing the `messages` attribute. For the License Recommendation Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an expert DFSA Licensing Assistant...'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "license_prompt_template.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can see that each tuple provided when using `ChatPromptTemplate.from_messages` becomes an individual prompt template itself. Within each of these tuples, the first value defines the _role_ of the message, which is typically `system`, `human`, or `ai`. For the License Recommendation Agent, we use explicit template definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# Explicit License Recommendation Template\n",
    "license_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])\n",
    "\n",
    "# Explicit RAG License Template\n",
    "rag_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(rag_prompt),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the structure of this new chat prompt template is identical to our previous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'query'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.                 \\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the License Recommendation Agent with Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've defined our prompt templates for the License Recommendation Agent. Now let's initialize the Azure OpenAI LLM and create a pipeline to process license recommendation queries.\n",
    "\n",
    "The License Recommendation Agent uses Azure OpenAI for backend processing. Ensure your Azure OpenAI credentials are configured in your environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Azure OpenAI Configuration\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\") or getpass(\"Enter Azure OpenAI Endpoint: \")\n",
    "azure_key = os.getenv(\"AZURE_OPENAI_API_KEY\") or getpass(\"Enter Azure OpenAI API Key: \")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4-32k\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Initialize Azure OpenAI LLM for License Recommendations\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0.0,  # Deterministic for accurate license recommendations\n",
    "    azure_deployment=deployment_name,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=azure_key,\n",
    "    api_version=api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the License Recommendation Agent, we use `temperature=0.0` to ensure deterministic, accurate recommendations. This prevents hallucinations and ensures compliance with DFSA regulations.\n",
    "\n",
    "We create a pipeline by connecting our `license_prompt_template` and `llm` using **L**ang**C**hain **E**xpression **L**anguage (LCEL), which uses the `|` operator to chain components together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create License Recommendation Pipeline\n",
    "license_pipeline = license_prompt_template | llm\n",
    "\n",
    "# Create RAG-enabled License Recommendation Pipeline\n",
    "rag_license_pipeline = rag_license_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a user query and relevant DFSA licensing context, then invoke our pipeline to get a license recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFSA License Database Context\n",
    "dfsa_context = \"\"\"DFSA License Types:\n",
    "1. Financial Services License: For investment services, wealth management, and financial advisory.\n",
    "   - Minimum Capital: AED 500,000\n",
    "   - Eligibility: Regulated entity with compliance framework\n",
    "\n",
    "2. Insurance License: For insurance brokers, agents, and underwriters.\n",
    "   - Minimum Capital: AED 250,000\n",
    "   - Eligibility: Insurance industry experience required\n",
    "\n",
    "3. Freezone License: For businesses operating in DIFC (Dubai International Financial Centre).\n",
    "   - Minimum Capital: AED 100,000\n",
    "   - Eligibility: Any business sector eligible\n",
    "\n",
    "4. Crowdfunding License: For peer-to-peer lending and crowdfunding platforms.\n",
    "   - Minimum Capital: AED 1,000,000\n",
    "   - Eligibility: Technology platform with compliance systems\n",
    "\"\"\"\n",
    "\n",
    "# Example user query\n",
    "user_query = \"I run a financial services company with AED 600,000 in capital. What license do I need?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on your financial services company with AED 600,000 in capital, I recommend the Financial Services License. Your capital exceeds the minimum requirement of AED 500,000, making you eligible. Next steps: 1) Prepare compliance framework documentation, 2) Submit application to DFSA, 3) Undergo regulatory review, 4) Receive license approval.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the License Recommendation Pipeline\n",
    "license_pipeline.invoke({\"query\": user_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our LLM pipeline is able to consume the information from the `context` and use it to answer the user's `query`. Ofcourse, we would not usually be feeding in both a question and an answer into an LLM manually. Typically, the `context` would be retrieved from a vector database, via web search, or from elsewhere. We will cover this use-case in full and build a functional RAG pipeline in a future chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Prompting for License Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot prompting is valuable for the License Recommendation Agent to provide consistent, accurate recommendations. By providing examples of user queries and expected license recommendations, we can improve the agent's ability to handle similar cases.\n",
    "\n",
    "Let's set up few-shot prompting for license recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot example template for license recommendations\n",
    "license_example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{query}\"),\n",
    "    (\"ai\", \"{recommendation}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define examples of license recommendation queries and their expected recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "license_examples = [\n",
    "    {\n",
    "        \"query\": \"I run a financial services company with AED 600,000 in capital. What license do I need?\",\n",
    "        \"recommendation\": \"Financial Services License. Your capital exceeds the minimum requirement of AED 500,000. Next steps: 1) Prepare compliance framework, 2) Submit application to DFSA, 3) Undergo regulatory review.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"I'm an insurance broker looking to operate in Dubai. What license should I apply for?\",\n",
    "        \"recommendation\": \"Insurance License. Minimum capital requirement is AED 250,000. Ensure you have insurance industry experience and compliance systems in place.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"I want to start a tech startup in the freezone. What are my licensing options?\",\n",
    "        \"recommendation\": \"Freezone License is ideal for your tech startup. Minimum capital is AED 100,000. DIFC offers a business-friendly environment for technology companies.\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a FewShotChatMessagePromptTemplate for license recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: I run a financial services company with AED 600,000 in capital. What license do I need?\n",
      "AI: Financial Services License. Your capital exceeds the minimum requirement of AED 500,000. Next steps: 1) Prepare compliance framework, 2) Submit application to DFSA, 3) Undergo regulatory review.\n",
      "Human: I'm an insurance broker looking to operate in Dubai. What license should I apply for?\n",
      "AI: Insurance License. Minimum capital requirement is AED 250,000. Ensure you have insurance industry experience and compliance systems in place.\n",
      "Human: I want to start a tech startup in the freezone. What are my licensing options?\n",
      "AI: Freezone License is ideal for your tech startup. Minimum capital is AED 100,000. DIFC offers a business-friendly environment for technology companies.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "few_shot_license_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=license_example_prompt,\n",
    "    examples=license_examples,\n",
    ")\n",
    "# Display the formatted few-shot prompt\n",
    "print(few_shot_prompt.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this we can provide different sets of `examples` or even different individual `example_prompt` templates to the `FewShotChatMessagePromptTemplate` object to build our prompt structure. Let's try an real example where we might use few-shot prompting.\n",
    "\n",
    "### Few-Shot Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our tiny LLM limits it's ability, so when asking for specific behaviors or structured outputs it can struggle. For example, we'll ask the LLM to summarize the key points about Aurelio AI using markdown and bullet points. Let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Overview of Aurelio AI\n",
      "\n",
      "Aurelio AI is an AI company that specializes in developing tools and solutions for AI engineers, particularly in the realm of language AI. \n",
      "\n",
      "## Key Focus Areas\n",
      "- **Language AI**: Expertise in building AI agents and information retrieval systems.\n",
      "- **Open Source Frameworks**: Development of frameworks like Semantic Router and Semantic Chunkers.\n",
      "- **AI Platform**: Provides tooling for engineers to facilitate AI development.\n",
      "- **Development Services**: Assists organizations in bringing their AI technologies to market.\n",
      "\n",
      "## Achievements\n",
      "- Became LangChain Experts in September 2024, showcasing their proficiency in the LangChain ecosystem.\n",
      "\n",
      "In conclusion, Aurelio AI is dedicated to empowering AI engineers through innovative tools, frameworks, and expert services in the field of language AI.\n"
     ]
    }
   ],
   "source": [
    "new_system_prompt = \"\"\"\n",
    "Answer the user's query based on the context below.                 \n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "When doing so please\n",
    "provide the answers to the point helpful engaging tone  .\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template.messages[0].prompt.template = new_system_prompt\n",
    "\n",
    "out = pipeline.invoke({\"query\": query, \"context\": context}).content\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display our markdown nicely with `IPython` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Overview of Aurelio AI\n",
       "\n",
       "Aurelio AI is an AI company that specializes in developing tools and solutions for AI engineers, particularly in the realm of language AI. \n",
       "\n",
       "## Key Focus Areas\n",
       "- **Language AI**: Expertise in building AI agents and information retrieval systems.\n",
       "- **Open Source Frameworks**: Development of frameworks like Semantic Router and Semantic Chunkers.\n",
       "- **AI Platform**: Provides tooling for engineers to facilitate AI development.\n",
       "- **Development Services**: Assists organizations in bringing their AI technologies to market.\n",
       "\n",
       "## Achievements\n",
       "- Became LangChain Experts in September 2024, showcasing their proficiency in the LangChain ecosystem.\n",
       "\n",
       "In conclusion, Aurelio AI is dedicated to empowering AI engineers through innovative tools, frameworks, and expert services in the field of language AI."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not bad, but also not quite the format we wanted. We could try improving our initial prompting instructions, but when this doesn't work we can move on to our few-shot prompting. We want to build something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Answer the user's query based on the context below,                 }\n",
    "if you cannot answer the question using the                         }\n",
    "provided information answer with \"I don't know\"                     }\n",
    "                                                                    }--->  (Rules)\n",
    "Always answer in markdown format. When doing so please              }\n",
    "provide headers, short summaries, follow with bullet                }\n",
    "points, then conclude. Here are some examples:                      }\n",
    "\n",
    "\n",
    "User: Can you explain gravity?                                      }\n",
    "AI: ## Gravity                                                      }\n",
    "                                                                    }\n",
    "Gravity is one of the fundamental forces in the universe.           }\n",
    "                                                                    }\n",
    "### Discovery                                                       }--->  (Example 1)\n",
    "                                                                    }\n",
    "* Gravity was first discovered by...                                }\n",
    "                                                                    }\n",
    "**To conclude**, Gravity is a fascinating topic and has been...     }\n",
    "                                                                    }\n",
    "\n",
    "User: What is the capital of France?                                }\n",
    "AI: ## France                                                       }\n",
    "                                                                    }\n",
    "The capital of France is Paris.                                     }\n",
    "                                                                    }--->  (Example 2)\n",
    "### Origins                                                         }\n",
    "                                                                    }\n",
    "* The name Paris comes from the...                                  }\n",
    "                                                                    }\n",
    "**To conclude**, Paris is highly regarded as one of the...          }\n",
    "\n",
    "Context: {context}                                                  }--->  (Context)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already defined our `example_prompt` so now we just change our `examples` to use some examples of a user asking a question and the LLM answering in the exact markdown format we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Can you explain gravity?\",\n",
    "        \"output\": (\n",
    "            \"## Gravity\\n\\n\"\n",
    "            \"Gravity is one of the fundamental forces in the universe.\\n\\n\"\n",
    "            \"### Discovery\\n\\n\"\n",
    "            \"* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n\"\n",
    "            \"* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n\"\n",
    "            \"### In General Relativity\\n\\n\"\n",
    "            \"* Gravity is described as the curvature of spacetime.\\n\"\n",
    "            \"* The more massive an object is, the more it curves spacetime.\\n\"\n",
    "            \"* This curvature is what causes objects to fall towards each other.\\n\\n\"\n",
    "            \"### Gravitons\\n\\n\"\n",
    "            \"* Gravitons are hypothetical particles that mediate the force of gravity.\\n\"\n",
    "            \"* They have not yet been detected.\\n\\n\"\n",
    "            \"**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is the capital of France?\",\n",
    "        \"output\": (\n",
    "            \"## France\\n\\n\"\n",
    "            \"The capital of France is Paris.\\n\\n\"\n",
    "            \"### Origins\\n\\n\"\n",
    "            \"* The name Paris comes from the Latin word \\\"Parisini\\\" which referred to a Celtic people living in the area.\\n\"\n",
    "            \"* The Romans named the city Lutetia, which means \\\"the place where the river turns\\\".\\n\"\n",
    "            \"* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n\"\n",
    "            \"**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\\n\\n\"\n",
    "        )\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed these into our `FewShotChatMessagePromptTemplate` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our formatted prompt now looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Human: Can you explain gravity?\n",
       "AI: ## Gravity\n",
       "\n",
       "Gravity is one of the fundamental forces in the universe.\n",
       "\n",
       "### Discovery\n",
       "\n",
       "* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\n",
       "* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\n",
       "\n",
       "### In General Relativity\n",
       "\n",
       "* Gravity is described as the curvature of spacetime.\n",
       "* The more massive an object is, the more it curves spacetime.\n",
       "* This curvature is what causes objects to fall towards each other.\n",
       "\n",
       "### Gravitons\n",
       "\n",
       "* Gravitons are hypothetical particles that mediate the force of gravity.\n",
       "* They have not yet been detected.\n",
       "\n",
       "**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\n",
       "\n",
       "\n",
       "Human: What is the capital of France?\n",
       "AI: ## France\n",
       "\n",
       "The capital of France is Paris.\n",
       "\n",
       "### Origins\n",
       "\n",
       "* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\n",
       "* The Romans named the city Lutetia, which means \"the place where the river turns\".\n",
       "* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\n",
       "\n",
       "**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = few_shot_prompt.format()\n",
    "\n",
    "display(Markdown(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pull all of this together with our system prompt and final user query to create our final prompt and feed it into our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", new_system_prompt),\n",
    "    few_shot_prompt,\n",
    "    (\"user\", \"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now feed this back into our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Aurelio AI Overview\n",
       "\n",
       "Aurelio AI is an AI company focused on developing tools and services for AI engineers, particularly in the realm of language AI.\n",
       "\n",
       "### Key Areas of Focus\n",
       "\n",
       "- **Language AI**: Specializes in creating AI solutions that understand and generate human language.\n",
       "- **Open Source Frameworks**: Developed notable frameworks like Semantic Router and Semantic Chunkers.\n",
       "- **AI Platform**: Provides engineers with tools to facilitate AI development.\n",
       "- **Development Services**: Offers services to help organizations bring their AI technologies to market.\n",
       "\n",
       "### Expertise\n",
       "\n",
       "- The team has strong expertise in building AI agents.\n",
       "- They possess a solid background in information retrieval.\n",
       "\n",
       "### Recognition\n",
       "\n",
       "- Became LangChain Experts in September 2024, showcasing their proficiency in the LangChain ecosystem.\n",
       "\n",
       "**To conclude**, Aurelio AI is dedicated to empowering AI engineers through innovative tools, frameworks, and development services in the language AI space."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = prompt_template | llm\n",
    "out = pipeline.invoke({\"query\": query, \"context\": context}).content\n",
    "display(Markdown(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by adding a few examples to our prompt, ie _few-shot prompting_, we can get much more control over the exact structure of our LLM response. As the size of our LLMs increases, the ability of them to follow instructions becomes much greater and they tend to require less explicit prompting as we have shown here. However, even for SotA models like `gpt-4o` few-shot prompting is still a valid technique that can be used if the LLM is struggling to follow our intended instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take a look at one more commonly used prompting technique called _chain of thought_ (CoT). CoT is a technique that encourages the LLM to think through the problem step by step before providing an answer. The idea being that by breaking down the problem into smaller steps, the LLM is more likely to arrive at the correct answer and we are less likely to see hallucinations.\n",
    "\n",
    "To implement CoT we don't need any specific LangChain objects, instead we are simply modifying how we instruct our LLM within the system prompt. We will ask the LLM to list the problems that need to be solved, to solve each problem individually, and then to arrive at the final answer.\n",
    "\n",
    "Let's first test our LLM _without_ CoT prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "You MUST answer the question directly without any other\n",
    "text or explanation.\n",
    "\"\"\"\n",
    "\n",
    "no_cot_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", no_cot_system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays most LLMs are trained to use CoT prompting by default, so we actually need to instruct it not to do so for this example which is why we added `\"You MUST answer the question directly without any other text or explanation.\"` to our system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of keystrokes needed to type the numbers from 1 to 500 is 1,500.\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"How many keystrokes are needed to type the numbers from 1 to 500?\"\n",
    ")\n",
    "\n",
    "no_cot_pipeline = no_cot_prompt_template | llm\n",
    "no_cot_result = no_cot_pipeline.invoke({\"query\": query}).content\n",
    "print(no_cot_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual answer is `1392`, but the LLM _without_ CoT just hallucinates and gives us a guess. Now, we can add explicit CoT prompting to our system prompt to see if we can get a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain-of-thought prompt template\n",
    "cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "To answer the question, you must:\n",
    "\n",
    "- List systematically and in precise detail all\n",
    "  subproblems that need to be solved to answer the\n",
    "  question.\n",
    "- Solve each sub problem INDIVIDUALLY and in sequence.\n",
    "- Finally, use everything you have worked through to\n",
    "  provide the final answer.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "cot_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", cot_system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "cot_pipeline = cot_prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To determine how many keystrokes are needed to type the numbers from 1 to 500, we can break this problem down into several subproblems:\n",
       "\n",
       "1. **Count the numbers from 1 to 9**: These are single-digit numbers.\n",
       "2. **Count the numbers from 10 to 99**: These are two-digit numbers.\n",
       "3. **Count the numbers from 100 to 499**: These are three-digit numbers.\n",
       "4. **Count the number 500**: This is a specific case of a three-digit number.\n",
       "\n",
       "Now, let's solve each subproblem individually.\n",
       "\n",
       "### Subproblem 1: Count the numbers from 1 to 9\n",
       "- There are 9 numbers (1, 2, 3, 4, 5, 6, 7, 8, 9).\n",
       "- Each number requires 1 keystroke.\n",
       "- Total keystrokes = 9 numbers * 1 keystroke = **9 keystrokes**.\n",
       "\n",
       "### Subproblem 2: Count the numbers from 10 to 99\n",
       "- There are 90 numbers (10 to 99).\n",
       "- Each number requires 2 keystrokes.\n",
       "- Total keystrokes = 90 numbers * 2 keystrokes = **180 keystrokes**.\n",
       "\n",
       "### Subproblem 3: Count the numbers from 100 to 499\n",
       "- There are 400 numbers (100 to 499).\n",
       "- Each number requires 3 keystrokes.\n",
       "- Total keystrokes = 400 numbers * 3 keystrokes = **1200 keystrokes**.\n",
       "\n",
       "### Subproblem 4: Count the number 500\n",
       "- The number 500 requires 3 keystrokes.\n",
       "- Total keystrokes = **3 keystrokes**.\n",
       "\n",
       "### Final Calculation\n",
       "Now, we can sum all the keystrokes from each subproblem:\n",
       "- Keystrokes from 1 to 9: 9\n",
       "- Keystrokes from 10 to 99: 180\n",
       "- Keystrokes from 100 to 499: 1200\n",
       "- Keystrokes for 500: 3\n",
       "\n",
       "Total keystrokes = 9 + 180 + 1200 + 3 = **1392 keystrokes**.\n",
       "\n",
       "Thus, the total number of keystrokes needed to type the numbers from 1 to 500 is **1392**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cot_result = cot_pipeline.invoke({\"query\": query, \"context\": context}).content\n",
    "display(Markdown(cot_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get a much better result! Our LLM provides us with a final answer of `1392` which is correct. Finally, as mentioned most LLMs are now trained to use CoT prompting by default. So let's see what happens if we don't explicitly tell the LLM to use CoT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question, keep it concise to the point and engaging/friendly to user.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To calculate the total number of keystrokes needed to type the numbers from 1 to 500, we can break it down by the number of digits in the numbers.\n",
       "\n",
       "1. **Numbers from 1 to 9**: \n",
       "   - There are 9 numbers (1 to 9).\n",
       "   - Each number has 1 digit.\n",
       "   - Total keystrokes = 9 * 1 = 9.\n",
       "\n",
       "2. **Numbers from 10 to 99**: \n",
       "   - There are 90 numbers (10 to 99).\n",
       "   - Each number has 2 digits.\n",
       "   - Total keystrokes = 90 * 2 = 180.\n",
       "\n",
       "3. **Numbers from 100 to 499**: \n",
       "   - There are 400 numbers (100 to 499).\n",
       "   - Each number has 3 digits.\n",
       "   - Total keystrokes = 400 * 3 = 1200.\n",
       "\n",
       "4. **Number 500**: \n",
       "   - There is 1 number (500).\n",
       "   - It has 3 digits.\n",
       "   - Total keystrokes = 1 * 3 = 3.\n",
       "\n",
       "Now, we can sum all the keystrokes:\n",
       "\n",
       "- From 1 to 9: 9 keystrokes\n",
       "- From 10 to 99: 180 keystrokes\n",
       "- From 100 to 499: 1200 keystrokes\n",
       "- From 500: 3 keystrokes\n",
       "\n",
       "Total keystrokes = 9 + 180 + 1200 + 3 = 1392.\n",
       "\n",
       "Therefore, the total number of keystrokes needed to type the numbers from 1 to 500 is **1392**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = pipeline.invoke({\"query\": query}).content\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We almost get the _exact_ same result. The formatting isn't quite as nice but the CoT behavior is clearly there, and the LLM produces the correct final answer!\n",
    "\n",
    "CoT is useful not only for simple question-answering like this, but is also a fundamental component of many agentic systems which will often use CoT steps paired with tool use to solve very complex problems, this is what we see in OpenAI's current flagship model `o1`. We'll see later in the course how we can do this ourselves.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
